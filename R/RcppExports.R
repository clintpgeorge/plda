# Generated by using Rcpp::compileAttributes() -> do not edit by hand
# Generator token: 10BE3573-1514-4C36-9D1C-5A225CD40393

#' LDA: Collapsed Gibbs Sampler with Perplexity Computation
#'
#' This implements of the collapsed Gibbs sampler for the LDA model---a Markov
#' chain on \eqn{z}. To compute perplexity, it first
#' partitions each document in the corpus into two sets of words: (a) a test
#' set (held-out set) and (b) a training set, given a user defined
#' \code{test_set_share}. Then, it runs the Markov chain based on the training
#' set and computes perplexity for the held-out set.
#'
#' @param num_topics Number of topics in the corpus
#' @param vocab_size  Vocabulary size
#' @param docs_tf A list of corpus documents read from the Blei corpus using
#'             \code{\link{read_docs}} (term indices starts with 0)
#' @param alpha_h Hyperparameter for \eqn{\theta} sampling
#' @param eta_h Smoothing parameter for the \eqn{\beta} matrix
#' @param max_iter Maximum number of Gibbs iterations to be performed
#' @param burn_in Burn-in-period for the Gibbs sampler
#' @param spacing Spacing between the stored samples (to reduce correlation)
#' @param save_z if 0 the function does not save \eqn{z} samples
#' @param save_beta if 0 the function does not save \eqn{\beta} samples
#' @param save_theta if 0 the function does not save \eqn{\theta} samples
#' @param save_lp if 0 the function does not save computed log posterior for
#'                iterations
#' @param verbose from {0, 1, 2}
#' @param test_set_share proportion of the test words in each document. Must be
#'                       between 0. and 1.
#'
#' @return The Markov chain output as a list of
#'   \item{corpus_topic_counts}{corpus-level topic counts from last iteration
#'   of the Markov chain}
#'   \item{theta_counts}{document-level topic counts from last iteration
#'   of the Markov chain}
#'   \item{beta_counts}{topic word counts from last iteration of the Markov chain}
#'   \item{Z_samples}{\eqn{z} samples after the burn in period, if
#'   \code{save_z} is set}
#'   \item{theta_samples}{\eqn{\theta} samples after the burn in period, if
#'   \code{save_theta} is set}
#'   \item{beta_samples}{\eqn{\beta} samples after the burn in period, if
#'   \code{save_beta} is set}
#'   \item{log_posterior}{the log posterior (upto a constant multiplier) of
#'   the hidden variable \eqn{\psi = (\beta, \theta, z)} in the LDA model,
#'   if \code{save_lp} is set}
#'   \item{perplexity}{perplexity of the held-out words' set}
#'
#' @export
#'
#' @family MCMC
#'
lda_cgs <- function(num_topics, vocab_size, docs_tf, alpha_h, eta_h, max_iter, burn_in, spacing, save_z, save_theta, save_beta, save_lp, verbose, test_set_share) {
    .Call('_plda_lda_cgs', PACKAGE = 'plda', num_topics, vocab_size, docs_tf, alpha_h, eta_h, max_iter, burn_in, spacing, save_z, save_theta, save_beta, save_lp, verbose, test_set_share)
}

#' LDA: Collapsed Gibbs Sampler with Perplexity Computation
#'
#' This implements of the collapsed Gibbs sampler for the LDA model---a Markov
#' chain on \eqn{z}. To compute perplexity, it first
#' partitions each document in the corpus into two sets of words: (a) a test
#' set (held-out set) and (b) a training set, given a user defined
#' \code{test_set_share}. Then, it runs the Markov chain based on the training
#' set and computes perplexity for the held-out set.
#'
#' @param num_topics Number of topics in the corpus
#' @param vocab_size  Vocabulary size
#' @param docs_tf A list of corpus documents read from the Blei corpus using
#'             \code{\link{read_docs}} (term indices starts with 0)
#' @param alpha_h Hyperparameter for \eqn{\theta} sampling
#' @param eta_h Smoothing parameter for the \eqn{\beta} matrix
#' @param max_iter Maximum number of Gibbs iterations to be performed
#' @param burn_in Burn-in-period for the Gibbs sampler
#' @param spacing Spacing between the stored samples (to reduce correlation)
#' @param save_z if 0 the function does not save \eqn{z} samples
#' @param save_beta if 0 the function does not save \eqn{\beta} samples
#' @param save_theta if 0 the function does not save \eqn{\theta} samples
#' @param save_lp if 0 the function does not save computed log posterior for
#'                iterations
#' @param verbose from {0, 1, 2}
#' @param test_set_share proportion of the test words in each document. Must be
#'                       between 0. and 1.
#'
#' @return The Markov chain output as a list of
#'   \item{corpus_topic_counts}{corpus-level topic counts from last iteration
#'   of the Markov chain}
#'   \item{theta_counts}{document-level topic counts from last iteration
#'   of the Markov chain}
#'   \item{beta_counts}{topic word counts from last iteration of the Markov chain}
#'   \item{Z_samples}{\eqn{z} samples after the burn in period, if
#'   \code{save_z} is set}
#'   \item{theta_samples}{\eqn{\theta} samples after the burn in period, if
#'   \code{save_theta} is set}
#'   \item{beta_samples}{\eqn{\beta} samples after the burn in period, if
#'   \code{save_beta} is set}
#'   \item{log_posterior}{the log posterior (upto a constant multiplier) of
#'   the hidden variable \eqn{\psi = (\beta, \theta, z)} in the LDA model,
#'   if \code{save_lp} is set}
#'   \item{perplexity}{perplexity of the held-out words' set}
#'
#' @export
#'
#' @family MCMC
#'
lda_cgs_v2 <- function(num_topics, vocab_size, docs_tf, alpha_h, eta_h, max_iter, burn_in, spacing, save_z, save_theta, save_beta, save_lp, verbose, test_set_share, beta_indices, theta_indices) {
    .Call('_plda_lda_cgs_v2', PACKAGE = 'plda', num_topics, vocab_size, docs_tf, alpha_h, eta_h, max_iter, burn_in, spacing, save_z, save_theta, save_beta, save_lp, verbose, test_set_share, beta_indices, theta_indices)
}

#' LDA: Full Gibbs Sampler with Perplexity Computation
#'
#' Implements the Full Gibbs sampler for the LDA model---a Markov chain on 
#' \eqn{(\beta, \theta, z)}. To compute perplexity, it first
#' partitions each document in the corpus into two sets of words: (a) a test
#' set (held-out set) and (b) a training set, given a user defined
#' \code{test_set_share}. Then, it runs the Markov chain based on the training
#' set and computes perplexity for the held-out set.
#'
#' @param num_topics Number of topics in the corpus
#' @param vocab_size  Vocabulary size
#' @param docs_tf A list of corpus documents read from the Blei corpus using
#'             \code{\link{read_docs}} (term indices starts with 0)
#' @param alpha_h Hyperparameter for \eqn{\theta} sampling
#' @param eta_h Smoothing parameter for the \eqn{\beta} matrix
#' @param max_iter Maximum number of Gibbs iterations to be performed
#' @param burn_in Burn-in-period for the Gibbs sampler
#' @param spacing Spacing between the stored samples (to reduce correlation)
#' @param save_z if 0 the function does not save \eqn{z} samples
#' @param save_beta if 0 the function does not save \eqn{\beta} samples
#' @param save_theta if 0 the function does not save \eqn{\theta} samples
#' @param save_lp if 0 the function does not save computed log posterior for
#'                iterations
#' @param verbose from {0, 1, 2}
#' @param test_set_share proportion of the test words in each document. Must be
#'                       between 0. and 1.
#'
#' @return The Markov chain output as a list of
#'   \item{corpus_topic_counts}{corpus-level topic counts from last iteration
#'   of the Markov chain}
#'   \item{theta_counts}{document-level topic counts from last iteration
#'   of the Markov chain}
#'   \item{beta_counts}{topic word counts from last iteration of the Markov chain}
#'   \item{Z_samples}{\eqn{z} samples after the burn in period, if
#'   \code{save_z} is set}
#'   \item{theta_samples}{\eqn{\theta} samples after the burn in period, if
#'   \code{save_theta} is set}
#'   \item{beta_samples}{\eqn{\beta} samples after the burn in period, if
#'   \code{save_beta} is set}
#'   \item{log_posterior}{the log posterior (upto a constant multiplier) of
#'   the hidden variable \eqn{\psi = (\beta, \theta, z)} in the LDA model,
#'   if \code{save_lp} is set}
#'   \item{perplexity}{perplexity of the held-out words' set}
#'
#' @export
#'
#' @family MCMC
#'
lda_fgs <- function(num_topics, vocab_size, docs_tf, alpha_h, eta_h, max_iter, burn_in, spacing, save_z, save_theta, save_beta, save_lp, verbose, test_set_share) {
    .Call('_plda_lda_fgs', PACKAGE = 'plda', num_topics, vocab_size, docs_tf, alpha_h, eta_h, max_iter, burn_in, spacing, save_z, save_theta, save_beta, save_lp, verbose, test_set_share)
}

#' LDA: Full Gibbs Sampler with Perplexity Computation
#' 
#' Note: The GGS implementation is slightly different to avoid an extra loop 
#' over the documents in a corpus. 
#'
#' Implements the Full Gibbs sampler for the LDA model---a Markov chain on 
#' \eqn{(\beta, \theta, z)}. To compute perplexity, it first
#' partitions each document in the corpus into two sets of words: (a) a test
#' set (held-out set) and (b) a training set, given a user defined
#' \code{test_set_share}. Then, it runs the Markov chain based on the training
#' set and computes perplexity for the held-out set.
#'
#' @param num_topics Number of topics in the corpus
#' @param vocab_size  Vocabulary size
#' @param docs_tf A list of corpus documents read from the Blei corpus using
#'             \code{\link{read_docs}} (term indices starts with 0)
#' @param alpha_h Hyperparameter for \eqn{\theta} sampling
#' @param eta_h Smoothing parameter for the \eqn{\beta} matrix
#' @param max_iter Maximum number of Gibbs iterations to be performed
#' @param burn_in Burn-in-period for the Gibbs sampler
#' @param spacing Spacing between the stored samples (to reduce correlation)
#' @param save_z if 0 the function does not save \eqn{z} samples
#' @param save_beta if 0 the function does not save \eqn{\beta} samples
#' @param save_theta if 0 the function does not save \eqn{\theta} samples
#' @param save_lp if 0 the function does not save computed log posterior for
#'                iterations
#' @param verbose from {0, 1, 2}
#' @param test_set_share proportion of the test words in each document. Must be
#'                       between 0. and 1.
#'
#' @return The Markov chain output as a list of
#'   \item{corpus_topic_counts}{corpus-level topic counts from last iteration
#'   of the Markov chain}
#'   \item{theta_counts}{document-level topic counts from last iteration
#'   of the Markov chain}
#'   \item{beta_counts}{topic word counts from last iteration of the Markov chain}
#'   \item{Z_samples}{\eqn{z} samples after the burn in period, if
#'   \code{save_z} is set}
#'   \item{theta_samples}{\eqn{\theta} samples after the burn in period, if
#'   \code{save_theta} is set}
#'   \item{beta_samples}{\eqn{\beta} samples after the burn in period, if
#'   \code{save_beta} is set}
#'   \item{log_posterior}{the log posterior (upto a constant multiplier) of
#'   the hidden variable \eqn{\psi = (\beta, \theta, z)} in the LDA model,
#'   if \code{save_lp} is set}
#'   \item{perplexity}{perplexity of the held-out words' set}
#'
#' @export
#'
#' @family MCMC
#'
lda_fgs_opt <- function(num_topics, vocab_size, docs_tf, alpha_h, eta_h, max_iter, burn_in, spacing, save_z, save_theta, save_beta, save_lp, verbose, test_set_share) {
    .Call('_plda_lda_fgs_opt', PACKAGE = 'plda', num_topics, vocab_size, docs_tf, alpha_h, eta_h, max_iter, burn_in, spacing, save_z, save_theta, save_beta, save_lp, verbose, test_set_share)
}

#' LDA: Full Gibbs Sampler with Perplexity Computation
#' 
#' Note: The GGS implementation is slightly different to avoid an extra loop 
#' over the documents in a corpus. 
#'
#' Implements the Full Gibbs sampler for the LDA model---a Markov chain on 
#' \eqn{(\beta, \theta, z)}. To compute perplexity, it first
#' partitions each document in the corpus into two sets of words: (a) a test
#' set (held-out set) and (b) a training set, given a user defined
#' \code{test_set_share}. Then, it runs the Markov chain based on the training
#' set and computes perplexity for the held-out set.
#'
#' @param num_topics Number of topics in the corpus
#' @param vocab_size  Vocabulary size
#' @param docs_tf A list of corpus documents read from the Blei corpus using
#'             \code{\link{read_docs}} (term indices starts with 0)
#' @param alpha_h Hyperparameter for \eqn{\theta} sampling
#' @param eta_h Smoothing parameter for the \eqn{\beta} matrix
#' @param max_iter Maximum number of Gibbs iterations to be performed
#' @param burn_in Burn-in-period for the Gibbs sampler
#' @param spacing Spacing between the stored samples (to reduce correlation)
#' @param save_z if 0 the function does not save \eqn{z} samples
#' @param save_beta if 0 the function does not save \eqn{\beta} samples
#' @param save_theta if 0 the function does not save \eqn{\theta} samples
#' @param save_lp if 0 the function does not save computed log posterior for
#'                iterations
#' @param verbose from {0, 1, 2}
#' @param test_set_share proportion of the test words in each document. Must be
#'                       between 0. and 1.
#'
#' @return The Markov chain output as a list of
#'   \item{corpus_topic_counts}{corpus-level topic counts from last iteration
#'   of the Markov chain}
#'   \item{theta_counts}{document-level topic counts from last iteration
#'   of the Markov chain}
#'   \item{beta_counts}{topic word counts from last iteration of the Markov chain}
#'   \item{Z_samples}{\eqn{z} samples after the burn in period, if
#'   \code{save_z} is set}
#'   \item{theta_samples}{\eqn{\theta} samples after the burn in period, if
#'   \code{save_theta} is set}
#'   \item{beta_samples}{\eqn{\beta} samples after the burn in period, if
#'   \code{save_beta} is set}
#'   \item{log_posterior}{the log posterior (upto a constant multiplier) of
#'   the hidden variable \eqn{\psi = (\beta, \theta, z)} in the LDA model,
#'   if \code{save_lp} is set}
#'   \item{perplexity}{perplexity of the held-out words' set}
#'
#' @export
#'
#' @family MCMC
#'
lda_fgs_opt_v2 <- function(num_topics, vocab_size, docs_tf, alpha_h, eta_h, max_iter, burn_in, spacing, save_z, save_theta, save_beta, save_lp, verbose, test_set_share, beta_indices, theta_indices) {
    .Call('_plda_lda_fgs_opt_v2', PACKAGE = 'plda', num_topics, vocab_size, docs_tf, alpha_h, eta_h, max_iter, burn_in, spacing, save_z, save_theta, save_beta, save_lp, verbose, test_set_share, beta_indices, theta_indices)
}

#' LDA: Partially-Collapsed Gibbs Sampler with Perplexity Computation
#'
#' This implements of the partially-collapsed Gibbs sampler for the LDA 
#' model---a Markov chain on \eqn{(z, \beta)}. To compute perplexity, it first
#' partitions each document in the corpus into two sets of words: (a) a test
#' set (held-out set) and (b) a training set, given a user defined
#' \code{test_set_share}. Then, it runs the Markov chain based on the training
#' set and computes perplexity for the held-out set.
#'
#' @param num_topics Number of topics in the corpus
#' @param vocab_size  Vocabulary size
#' @param docs_tf A list of corpus documents read from the Blei corpus using
#'             \code{\link{read_docs}} (term indices starts with 0)
#' @param alpha_h Hyperparameter for \eqn{\theta} sampling
#' @param eta_h Smoothing parameter for the \eqn{\beta} matrix
#' @param max_iter Maximum number of Gibbs iterations to be performed
#' @param burn_in Burn-in-period for the Gibbs sampler
#' @param spacing Spacing between the stored samples (to reduce correlation)
#' @param save_z if 0 the function does not save \eqn{z} samples
#' @param save_beta if 0 the function does not save \eqn{\beta} samples
#' @param save_theta if 0 the function does not save \eqn{\theta} samples
#' @param save_lp if 0 the function does not save computed log posterior for
#'                iterations
#' @param verbose from {0, 1, 2}
#' @param test_set_share proportion of the test words in each document. Must be
#'                       between 0. and 1.
#'
#' @return The Markov chain output as a list of
#'   \item{corpus_topic_counts}{corpus-level topic counts from last iteration
#'   of the Markov chain}
#'   \item{theta_counts}{document-level topic counts from last iteration
#'   of the Markov chain}
#'   \item{beta_counts}{topic word counts from last iteration of the Markov chain}
#'   \item{Z_samples}{\eqn{z} samples after the burn in period, if
#'   \code{save_z} is set}
#'   \item{theta_samples}{\eqn{\theta} samples after the burn in period, if
#'   \code{save_theta} is set}
#'   \item{beta_samples}{\eqn{\beta} samples after the burn in period, if
#'   \code{save_beta} is set}
#'   \item{log_posterior}{the log posterior (upto a constant multiplier) of
#'   the hidden variable \eqn{\psi = (\beta, \theta, z)} in the LDA model,
#'   if \code{save_lp} is set}
#'   \item{perplexity}{perplexity of the held-out words' set}
#'
#' @export
#'
#' @family MCMC
#'
lda_pcgs <- function(num_topics, vocab_size, docs_tf, alpha_h, eta_h, max_iter, burn_in, spacing, save_z, save_theta, save_beta, save_lp, verbose, test_set_share) {
    .Call('_plda_lda_pcgs', PACKAGE = 'plda', num_topics, vocab_size, docs_tf, alpha_h, eta_h, max_iter, burn_in, spacing, save_z, save_theta, save_beta, save_lp, verbose, test_set_share)
}

#' LDA: Partially-Collapsed Gibbs Sampler with Perplexity Computation
#'
#' This implements of the partially-collapsed Gibbs sampler for the LDA 
#' model---a Markov chain on \eqn{(z, \beta)}. To compute perplexity, it first
#' partitions each document in the corpus into two sets of words: (a) a test
#' set (held-out set) and (b) a training set, given a user defined
#' \code{test_set_share}. Then, it runs the Markov chain based on the training
#' set and computes perplexity for the held-out set.
#'
#' @param num_topics Number of topics in the corpus
#' @param vocab_size  Vocabulary size
#' @param docs_tf A list of corpus documents read from the Blei corpus using
#'             \code{\link{read_docs}} (term indices starts with 0)
#' @param alpha_h Hyperparameter for \eqn{\theta} sampling
#' @param eta_h Smoothing parameter for the \eqn{\beta} matrix
#' @param max_iter Maximum number of Gibbs iterations to be performed
#' @param burn_in Burn-in-period for the Gibbs sampler
#' @param spacing Spacing between the stored samples (to reduce correlation)
#' @param save_z if 0 the function does not save \eqn{z} samples
#' @param save_beta if 0 the function does not save \eqn{\beta} samples
#' @param save_theta if 0 the function does not save \eqn{\theta} samples
#' @param save_lp if 0 the function does not save computed log posterior for
#'                iterations
#' @param verbose from {0, 1, 2}
#' @param test_set_share proportion of the test words in each document. Must be
#'                       between 0. and 1.
#'
#' @return The Markov chain output as a list of
#'   \item{corpus_topic_counts}{corpus-level topic counts from last iteration
#'   of the Markov chain}
#'   \item{theta_counts}{document-level topic counts from last iteration
#'   of the Markov chain}
#'   \item{beta_counts}{topic word counts from last iteration of the Markov chain}
#'   \item{Z_samples}{\eqn{z} samples after the burn in period, if
#'   \code{save_z} is set}
#'   \item{theta_samples}{\eqn{\theta} samples after the burn in period, if
#'   \code{save_theta} is set}
#'   \item{beta_samples}{\eqn{\beta} samples after the burn in period, if
#'   \code{save_beta} is set}
#'   \item{log_posterior}{the log posterior (upto a constant multiplier) of
#'   the hidden variable \eqn{\psi = (\beta, \theta, z)} in the LDA model,
#'   if \code{save_lp} is set}
#'   \item{perplexity}{perplexity of the held-out words' set}
#'
#' @export
#'
#' @family MCMC
#'
lda_pcgs_v2 <- function(num_topics, vocab_size, docs_tf, alpha_h, eta_h, max_iter, burn_in, spacing, save_z, save_theta, save_beta, save_lp, verbose, test_set_share, beta_indices, theta_indices) {
    .Call('_plda_lda_pcgs_v2', PACKAGE = 'plda', num_topics, vocab_size, docs_tf, alpha_h, eta_h, max_iter, burn_in, spacing, save_z, save_theta, save_beta, save_lp, verbose, test_set_share, beta_indices, theta_indices)
}

#' Samples from the Antoniak distribution
#'
#' It's done by sampling \eqn{N} Bernoulli variables
#'
#' References:
#'
#'   http://www.jmlr.org/papers/volume10/newman09a/newman09a.pdf
#'
#' @param N Number of samples
#' @param alpha strength parameter
#'
#' @export
#'
#' @family utils
#'
#' @note
#'
#' Created on: May 19, 2016
#'
#' Created by: Clint P. George
#'
sample_antoniak <- function(N, alpha) {
    .Call('_plda_sample_antoniak', PACKAGE = 'plda', N, alpha)
}

#' A speedy sampling from a multimomial distribution
#'
#' @param theta a multinomial probability vector (K x 1 vector)
#'
#' @return returns a class index from [0, K)
#'
#' @note
#' Author: Clint P. George
#'
#' Created on: February 11, 2016
#'
#' @family utils
#'
#' @export
sample_multinomial <- function(theta) {
    .Call('_plda_sample_multinomial', PACKAGE = 'plda', theta)
}

#' Samples from a Dirichlet distribution given a hyperparameter
#'
#' @param num_elements the dimention of the Dirichlet distribution
#' @param alpha the hyperparameter vector (a column vector)
#'
#' @return returns a Dirichlet sample (a column vector)
#'
#' @note
#' Author: Clint P. George
#'
#' Created on: 2014
#'
#' @family utils
#'
#' @export
sample_dirichlet <- function(num_elements, alpha) {
    .Call('_plda_sample_dirichlet', PACKAGE = 'plda', num_elements, alpha)
}

